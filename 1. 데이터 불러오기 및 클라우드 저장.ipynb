{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 팀프로젝트4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine, inspect\n",
    "from google.cloud import storage, bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MySQL에서 DB에서 table을 가져와서 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mysql에서 데이터를 가져와서 확인만 하는 용도\n",
    "\n",
    "# MySQL 연결 설정\n",
    "db_user = \"\"\n",
    "db_password = \"!\"\n",
    "db_host = \"\"  # 또는 MySQL 호스트 주소\n",
    "db_port = \"\"       # 기본 포트\n",
    "databases = [\"A\", \"B\"]\n",
    "\n",
    "\n",
    "# 결과 저장용 딕셔너리\n",
    "all_dataframes = {}\n",
    "\n",
    "# 데이터베이스별로 처리\n",
    "for db_name in databases:\n",
    "    print(f\"Processing database: {db_name}\")\n",
    "    # SQLAlchemy 엔진 생성\n",
    "    engine = create_engine(f\"mysql+pymysql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}\")\n",
    "    \n",
    "    try:\n",
    "        # 테이블 이름 가져오기\n",
    "        inspector = inspect(engine)\n",
    "        table_names = inspector.get_table_names()\n",
    "        \n",
    "        # 각 테이블 데이터 가져오기\n",
    "        db_dataframes = {}\n",
    "        for table in table_names:\n",
    "            query = f\"SELECT * FROM {table}\"\n",
    "            with engine.connect() as connection:\n",
    "                db_dataframes[table] = pd.read_sql(query, con=connection)\n",
    "                print(f\"Loaded table '{table}' from database '{db_name}'.\")\n",
    "        \n",
    "        # 결과 저장\n",
    "        all_dataframes[db_name] = db_dataframes\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing database {db_name}: {e}\")\n",
    "    finally:\n",
    "        # 엔진 닫기\n",
    "        engine.dispose()\n",
    "\n",
    "# 결과 확인\n",
    "for db_name, tables in all_dataframes.items():\n",
    "    print(f\"Database: {db_name}\")\n",
    "    for table_name, df in tables.items():\n",
    "        print(f\"Table: {table_name} (Rows: {len(df)})\")\n",
    "        print(df.head(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MySQL DB에서 데이터 추출 후 로컬에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### mysql에서 로컬로 데이터\n",
    "\n",
    "# MySQL 연결 설정\n",
    "db_user = \"\"\n",
    "db_password = \"!\"\n",
    "db_host = \"\"  # 또는 MySQL 호스트 주소\n",
    "db_port = \"\"       # 기본 포트\n",
    "databases = [\"A\", \"B\"]\n",
    "\n",
    "# 저장 경로 설정\n",
    "output_dir = \"./origin_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # 저장 디렉토리 생성\n",
    "\n",
    "# 데이터베이스별로 처리\n",
    "for db_name in databases:\n",
    "    print(f\"Processing database: {db_name}\")\n",
    "    # SQLAlchemy 엔진 생성\n",
    "    engine = create_engine(f\"mysql+pymysql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}\")\n",
    "    \n",
    "    try:\n",
    "        # 테이블 이름 가져오기\n",
    "        inspector = inspect(engine)\n",
    "        table_names = inspector.get_table_names()\n",
    "        \n",
    "        # 각 테이블 데이터 가져오기\n",
    "        for table in table_names:\n",
    "            query = f\"SELECT * FROM {table}\"\n",
    "            with engine.connect() as connection:\n",
    "                df = pd.read_sql(query, con=connection)\n",
    "                \n",
    "                # Parquet 파일로 저장\n",
    "                parquet_file = os.path.join(output_dir, f\"{db_name}_{table}.parquet\")\n",
    "                df.to_parquet(parquet_file, index=False)\n",
    "                print(f\"Saved table '{table}' from database '{db_name}' to '{parquet_file}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing database {db_name}: {e}\")\n",
    "    finally:\n",
    "        # 엔진 닫기\n",
    "        engine.dispose()\n",
    "\n",
    "print(\"\\nData extraction and Parquet file saving completed.\")\n",
    "\n",
    "# 저장된 Parquet 파일 읽기\n",
    "print(\"\\nLoading Parquet files:\")\n",
    "parquet_files = [f for f in os.listdir(output_dir) if f.endswith(\".parquet\")]\n",
    "for parquet_file in parquet_files:\n",
    "    full_path = os.path.join(output_dir, parquet_file)\n",
    "    df = pd.read_parquet(full_path)\n",
    "    print(f\"Loaded {parquet_file} (Rows: {len(df)})\")\n",
    "    print(df.head(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data 파일 용량 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 용량 확인\n",
    "def get_csv_file_sizes(directory):\n",
    "    file_sizes = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.parquet'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            file_size_bytes = os.path.getsize(file_path)\n",
    "            file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "            file_sizes[filename] = file_size_mb\n",
    "    return file_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# directory = '데이터가 저장된 위치'\n",
    "directory = './origin_data'\n",
    "csv_file_sizes = get_csv_file_sizes(directory)\n",
    "sorted_dict = dict(sorted(csv_file_sizes.items(), key=lambda item: item[1], reverse=True))\n",
    "for filename, size in sorted_dict.items():\n",
    "    print(f'{filename:<50}: {size:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 미리보기\n",
    "- 지정된 dataset이 있는 폴더내의 parquet파일을 전부 불러서 확인.\n",
    "- pyarrow engine이 인식속도가 가장 빠르다고 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunk_size = 100000\n",
    "\n",
    "# Parquet 파일을 청크 단위로 읽어오는 예제\n",
    "for filename in sorted_dict.keys():\n",
    "    each_path = directory + '/' + filename\n",
    "    df = pd.read_parquet(each_path, engine='pyarrow')\n",
    "    # 청크 단위로 처리할 필요 없이, Parquet 파일은 기본적으로 전체 데이터를 한 번에 읽어옵니다.\n",
    "    print(each_path.split('/')[-1], df.shape)\n",
    "    display(df.head(5))\n",
    "    display(df.dtypes)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### local(data 폴더에서) -> GCS(Bucket) 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def upload_parquet_to_gcs(data_type, bucket_name, json_key_file, local_folder):\n",
    "    \"\"\"\n",
    "    GCS에 parquet 파일 업로드\n",
    "    :param bucket_name: GCS 버킷 이름\n",
    "    :param source_folder: 업로드할 로컬 폴더 경로\n",
    "    :param json_key_file: GCS 서비스 계정 JSON 키 파일 경로\n",
    "    :param destination_folder: GCS 내 저장 폴더 경로 (선택)\n",
    "    \"\"\"\n",
    "    # GCS 클라이언트 생성\n",
    "    client = storage.Client.from_service_account_json(json_key_file)\n",
    "    bucket = client.bucket(bucket_name)\n",
    "\n",
    "    for file_name in os.listdir(local_folder):\n",
    "        if file_name.endswith(\".parquet\"):\n",
    "            # 데이터베이스 이름과 테이블 이름 추출\n",
    "            base_name, _ = os.path.splitext(file_name)  # 확장자 제거\n",
    "            db_name, table_name = base_name.split(\"_\", 1)  # 첫 '_' 기준 분리\n",
    "\n",
    "            # GCS 폴더 경로 지정\n",
    "            destination_blob_name = f\"{data_type}/{db_name}/{table_name}.parquet\"\n",
    "\n",
    "            # GCS 업로드\n",
    "            blob = bucket.blob(destination_blob_name)\n",
    "            blob.upload_from_filename(os.path.join(local_folder, file_name))\n",
    "\n",
    "            print(f\"Uploaded {file_name} to gs://{bucket_name}/{destination_blob_name}\")\n",
    "\n",
    "# 실행 예제\n",
    "data_type = \"A\"\n",
    "bucket_name = \"Bucket\"  # GCS 버킷 이름\n",
    "source_folder = \"./\"      # 데이터가 저장된 폴더위치\n",
    "json_key_file = \"./config/gcp_key.json\"   # JSON 인증 파일 경로\n",
    "upload_parquet_to_gcs(data_type, bucket_name, json_key_file, source_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCS에서 데이터 불러오기\n",
    "- 불러온 데이터 .temp라는 임시폴더에 데이터 저장(로컬형태로 함!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 환경 변수에 JSON 키 파일 설정\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./config/gcp_key.json\"\n",
    "\n",
    "# GCS 클라이언트 및 BigQuery 클라이언트 초기화\n",
    "storage_client = storage.Client()\n",
    "bigquery_client = bigquery.Client()\n",
    "\n",
    "def download_parquet_from_gcs(bucket_name, prefix):\n",
    "    \"\"\"\n",
    "    GCS에서 Parquet 파일 다운로드 및 병합.\n",
    "    :param bucket_name: GCS 버킷 이름\n",
    "    :param prefix: 다운로드할 경로 (GCS 버킷 내부 폴더)\n",
    "    :return: 병합된 Pandas 데이터프레임\n",
    "    \"\"\"\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs(prefix=prefix)  # 지정된 경로의 파일 검색\n",
    "    dfs = []  # 데이터프레임 저장 리스트\n",
    "\n",
    "    for blob in blobs:\n",
    "        if blob.name.endswith(\".parquet\"):\n",
    "            print(f\"Downloading: {blob.name}\")\n",
    "            local_file_path = f\"./temp/{blob.name.split('/')[-1]}\"  # 로컬 임시 저장 경로\n",
    "            os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "            blob.download_to_filename(local_file_path)  # 파일 다운로드\n",
    "            df = pd.read_parquet(local_file_path)  # Parquet 파일 읽기\n",
    "            dfs.append(df)\n",
    "\n",
    "    if not dfs:\n",
    "        print(f\"No Parquet files found at prefix: {prefix}\")\n",
    "        return pd.DataFrame()  # 빈 데이터프레임 반환\n",
    "\n",
    "    return pd.concat(dfs, ignore_index=True)  # 데이터프레임 병합\n",
    "\n",
    "# GCS에서 Parquet 데이터 다운로드\n",
    "# 실행 예제\n",
    "if __name__ == \"__main__\":\n",
    "    bucket_name = \"finalproject_sprint\"\n",
    "    prefix = \"hackle\"  # GCS 내부의 특정 경로(버킷에서 파일이 저장된 폴더 이름.)\n",
    "    dataset_name = \"your_dataset_name\"\n",
    "    table_name = \"your_table_name\"\n",
    "\n",
    "    # GCS에서 Parquet 데이터 다운로드\n",
    "    merged_df = download_parquet_from_gcs(bucket_name, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 빅쿼리에 데이터 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def upload_to_bigquery(dataframe, dataset_name, table_name):\n",
    "    \"\"\"\n",
    "    Pandas 데이터프레임을 BigQuery 테이블에 업로드.\n",
    "    :param dataframe: 업로드할 데이터프레임\n",
    "    :param dataset_name: BigQuery 데이터셋 이름\n",
    "    :param table_name: BigQuery 테이블 이름\n",
    "    \"\"\"\n",
    "    table_id = f\"{bigquery_client.project}.{dataset_name}.{table_name}\"\n",
    "    job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")  # 기존 데이터 덮어쓰기\n",
    "    job = bigquery_client.load_table_from_dataframe(dataframe, table_id, job_config=job_config)\n",
    "    job.result()  # 작업 완료 대기\n",
    "\n",
    "    print(f\"Uploaded {len(dataframe)} rows to {table_id}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 빅쿼리 데이터 업로드 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    bucket_name = \"your_gcs_bucket_name\"\n",
    "    prefix = \"dict\"  # GCS 내부의 특정 경로\n",
    "    dataset_name = \"your_dataset_name\"\n",
    "    table_name = \"your_table_name\"\n",
    "\n",
    "    # GCS에서 Parquet 데이터 다운로드\n",
    "    merged_df = download_parquet_from_gcs(bucket_name, prefix)\n",
    "\n",
    "    if not merged_df.empty:\n",
    "        # BigQuery로 데이터 업로드\n",
    "        upload_to_bigquery(merged_df, dataset_name, table_name)\n",
    "    else:\n",
    "        print(\"No data to upload.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCS로 부터 parquet파일 받아오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from google.cloud import storage, bigquery\n",
    "\n",
    "# 환경 변수에 JSON 키 파일 설정\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./config/gcp_key.json\"\n",
    "\n",
    "# GCS 클라이언트 및 BigQuery 클라이언트 초기화\n",
    "storage_client = storage.Client()\n",
    "bigquery_client = bigquery.Client()\n",
    "\n",
    "def download_parquet_from_gcs(bucket_name, prefix):\n",
    "    \"\"\"\n",
    "    GCS에서 Parquet 파일 다운로드 및 병합.\n",
    "    :param bucket_name: GCS 버킷 이름\n",
    "    :param prefix: 다운로드할 경로 (GCS 버킷 내부 폴더)\n",
    "    :return: 파일별 데이터프레임 딕셔너리 (key: 파일 이름, value: 데이터프레임)\n",
    "    \"\"\"\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs(prefix=prefix)  # 지정된 경로의 파일 검색\n",
    "    dataframes = {}  # 파일별 데이터프레임 저장\n",
    "    file_names = []\n",
    "\n",
    "    for blob in blobs:\n",
    "        if blob.name.endswith(\".parquet\"):\n",
    "            file_name = blob.name.split(\"/\")[-1].replace(\".parquet\", \"\")  # 파일 이름 추출\n",
    "            print(f\"Downloading: {blob.name}\")\n",
    "            # GCS에서 바로 메모리로 읽기\n",
    "            with blob.open(\"rb\") as file:\n",
    "                df = pd.read_parquet(file, engine=\"pyarrow\")\n",
    "                dataframes[file_name] = df\n",
    "                file_names.append(file_name)\n",
    "\n",
    "    if not dataframes:\n",
    "        print(f\"No Parquet files found at prefix: {prefix}\")\n",
    "    return dataframes, file_names  # 파일 이름별 데이터프레임 딕셔너리 반환\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCS에서 가져온 데이터 Bigquery로 이관"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def upload_to_bigquery(dataframe, dataset_name, table_name):\n",
    "    \"\"\"\n",
    "    Pandas 데이터프레임을 BigQuery 테이블에 업로드.\n",
    "    :param dataframe: 업로드할 데이터프레임\n",
    "    :param dataset_name: BigQuery 데이터셋 이름\n",
    "    :param table_name: BigQuery 테이블 이름\n",
    "    \"\"\"\n",
    "    table_id = f\"{bigquery_client.project}.{dataset_name}.{table_name}\"\n",
    "    job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")  # 기존 데이터 덮어쓰기\n",
    "    job = bigquery_client.load_table_from_dataframe(dataframe, table_id, job_config=job_config)\n",
    "    job.result()  # 작업 완료 대기\n",
    "    print(f\"Uploaded {len(dataframe)} rows to {table_id}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 실행, prefix, dataset_name은 동일하게 들어갈 것 같습니다.\n",
    "if __name__ == \"__main__\":\n",
    "    db_name = \"H2\"\n",
    "    bucket_name = \"finalproject_sprint\" \n",
    "    prefix = db_name  # GCS 내부의 특정 경로(버킷에서 파일이 저장된 폴더 이름.)\n",
    "    dataset_name = db_name  # 데이터셋 이름 지정\n",
    "\n",
    "    # GCS에서 Parquet 데이터 다운로드\n",
    "    dataframes = download_parquet_from_gcs(bucket_name, prefix)\n",
    "\n",
    "    # 각 파일을 BigQuery로 업로드\n",
    "    for file_name, df in dataframes.items():\n",
    "        table_name = file_name  # 파일 이름을 테이블 이름으로 사용\n",
    "        print(f\"Uploading file '{file_name}' to BigQuery table '{table_name}'...\")\n",
    "        upload_to_bigquery(df, dataset_name, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 그냥 데이터 다운로드만 하고 확인 할 때 사용.\n",
    "## 실행, prefix, dataset_name은 동일하게 들어갈 것 같습니다.\n",
    "if __name__ == \"__main__\":\n",
    "    db_name = \"H2\"\n",
    "    bucket_name = \"finalproject_sprint\"\n",
    "    prefix = db_name  # GCS 내부의 특정 경로(버킷에서 파일이 저장된 폴더 이름.)\n",
    "    dataset_name = db_name  # 데이터셋 이름 지정\n",
    "\n",
    "    # GCS에서 Parquet 데이터 다운로드\n",
    "    dataframes, file_names = download_parquet_from_gcs(bucket_name, prefix)\n",
    "    print(f\"DB name : {db_name}, table_name : {file_names}\")\n",
    "\n",
    "    # GCS에서 받은 parquet의 file이름으로 데이터 저장\n",
    "    for file_name in file_names:\n",
    "        globals()[file_name] = dataframes[f'{file_name}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hackle_events = dataframes['hackle_events']\n",
    "\n",
    "## 조금 중요한데 만약에 forloop를 이용해서 df의 len만큼 돌린다면 매우 큰 과부화가 걸릴 수 있음.\n",
    "## 벡터화 연산을 이용하면 빠르게 변환이 가능. 혹은 np.arange로 동일하게 처리하는 방법을 사용!\n",
    "hackle_events['id'] = range(1, len(hackle_events) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(hackle_events.head())\n",
    "print('hackle_events table shape is : ', hackle_events.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hackle_events.to_parquet('parquet_data/hackle_hackle_events.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hackle_events['session_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hackle_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_properties['user_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hackle_properties = dataframes['hackle_properties']\n",
    "display(hackle_properties.head())\n",
    "print(\"hackle_properties의 테이블 크기 : \", hackle_properties.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(set(hackle_properties['session_id']).intersection(set(hackle_properties['device_id'])))\n",
    "\n",
    "len(set(hackle_properties['session_id']).intersection(set(hackle_properties['device_id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hackle_properties['session_id'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codeit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
